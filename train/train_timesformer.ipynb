{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e65ba7",
   "metadata": {},
   "source": [
    "## üîµ STEP 1 ‚Äî ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è ÎùºÏù¥Î∏åÎü¨Î¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless\n",
    "!pip install scikit-learn\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import TimesformerConfig, TimesformerModel\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_FRAMES  = 16\n",
    "IMG_SIZE    = 224\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE  = 2\n",
    "EPOCHS      = 15\n",
    "LR          = 1e-4\n",
    "\n",
    "TRAIN_DIR = \"/workspace/data/train\"\n",
    "VAL_DIR   = \"/workspace/data/val\"\n",
    "SAVE_PATH = \"/workspace/model/best_model.pth\"\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Train dir:\", TRAIN_DIR)\n",
    "print(\"Val dir:\", VAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a3012",
   "metadata": {},
   "source": [
    "## üîµ STEP 2 ‚Äî Ï†ÑÏ≤òÎ¶¨ Ï†ïÏùò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2,\n",
    "                               saturation=0.2, hue=0.02)\n",
    "    ], p=0.38),\n",
    "    transforms.RandomApply(\n",
    "        [transforms.GaussianBlur(3, sigma=(0.1, 1.0))], p=0.15\n",
    "    ),\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomAdjustSharpness(1.3)], p=0.16\n",
    "    ),\n",
    "    transforms.RandomGrayscale(p=0.05),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d15171",
   "metadata": {},
   "source": [
    "## üîµ STEP 3 ‚Äî Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ab91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root, num_frames, transform):\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "        self.class_to_idx = {\"real\": 0, \"fake\": 1}\n",
    "\n",
    "        for cls in [\"real\", \"fake\"]:\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "\n",
    "            for vid_dir in os.listdir(cls_dir):\n",
    "                full = os.path.join(cls_dir, vid_dir)\n",
    "                if os.path.isdir(full):\n",
    "                    self.files.append((full, self.class_to_idx[cls]))\n",
    "\n",
    "        print(f\"[VideoDataset] root={root}, samples={len(self.files)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def load_frames(self, folder):\n",
    "        # Ìè¥Îçî ÏïàÏùò jpg Ï†ïÎ†¨Ìï¥ÏÑú ÏÇ¨Ïö©\n",
    "        jpgs = sorted(glob(os.path.join(folder, \"*.jpg\")))\n",
    "\n",
    "        if len(jpgs) == 0:\n",
    "            # ÎπÑÏñ¥ ÏûàÏúºÎ©¥ Í∑∏ÎÉ• Í≤ÄÏùÄ ÌôîÎ©¥ Ìå®Îî© (ÏóêÎü¨ Î∞©ÏßÄÏö©)\n",
    "            dummy = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE))\n",
    "            imgs = [self.transform(dummy) for _ in range(self.num_frames)]\n",
    "            return torch.stack(imgs)\n",
    "\n",
    "        # ÌîÑÎ†àÏûÑ Î∂ÄÏ°±ÌïòÎ©¥ ÎßàÏßÄÎßâ ÌîÑÎ†àÏûÑ Î∞òÎ≥µÌï¥ÏÑú Ìå®Îî©\n",
    "        if len(jpgs) < self.num_frames:\n",
    "            jpgs = jpgs + [jpgs[-1]] * (self.num_frames - len(jpgs))\n",
    "\n",
    "        # ÎÑàÎ¨¥ ÎßéÏúºÎ©¥ ÏïûÏóêÏÑúÎ∂ÄÌÑ∞ 16Ïû•Îßå ÏÇ¨Ïö© (ÌïÑÏöîÌïòÎ©¥ Ïä¨ÎùºÏù¥Îî©/ÎûúÎç§ÏúºÎ°ú Î∞îÍøÄ Ïàò ÏûàÏùå)\n",
    "        jpgs = jpgs[:self.num_frames]\n",
    "\n",
    "        frames = []\n",
    "        for p in jpgs:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            frames.append(self.transform(img))\n",
    "\n",
    "        # [T, 3, H, W]\n",
    "        return torch.stack(frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder, label = self.files[idx]\n",
    "        frames = self.load_frames(folder)      # [T, 3, 224, 224]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return frames, label\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Dataloader\n",
    "# ==========================\n",
    "train_dataset = VideoDataset(TRAIN_DIR, NUM_FRAMES, train_transform)\n",
    "val_dataset   = VideoDataset(VAL_DIR,   NUM_FRAMES, val_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae80996",
   "metadata": {},
   "source": [
    "## üîµ STEP 4 ‚Äî TimeSformer Î™®Îç∏ Ï†ïÏùò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSformerWrapper(nn.Module):\n",
    "    def __init__(self, num_classes=2, num_frames=16, img_size=224):\n",
    "        super().__init__()\n",
    "\n",
    "        config = TimesformerConfig(\n",
    "            num_frames=num_frames,\n",
    "            num_labels=num_classes,\n",
    "            image_size=img_size,\n",
    "\n",
    "            patch_size=16,\n",
    "            attention_type=\"divided_space_time\",\n",
    "\n",
    "            num_hidden_layers=8,\n",
    "            hidden_size=768,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072,\n",
    "\n",
    "            dropout=0.1,\n",
    "            attention_dropout=0.1,\n",
    "        )\n",
    "\n",
    "        self.backbone = TimesformerModel(config)\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(config.hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "        for m in self.cls_head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, 3, H, W]\n",
    "        out = self.backbone(pixel_values=x)\n",
    "        # [B, num_tokens, hidden] ‚Üí ÌèâÍ∑† ÌíÄÎßÅ\n",
    "        pooled = out.last_hidden_state.mean(dim=1)\n",
    "        return self.cls_head(pooled)\n",
    "\n",
    "\n",
    "model = TimeSformerWrapper(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_frames=NUM_FRAMES,\n",
    "    img_size=IMG_SIZE\n",
    ").to(device)\n",
    "\n",
    "print(model.__class__.__name__, \"initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b987c",
   "metadata": {},
   "source": [
    "## üîµ STEP 5 ‚Äî Optimizer / Scheduler / Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bebd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=LR, weight_decay=5e-5\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba973945",
   "metadata": {},
   "source": [
    "## üîµ STEP 6 ‚Äî Train / Validation / Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34217180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for frames, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        # frames: [B, T, 3, 224, 224]\n",
    "        frames = frames.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    preds, gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            frames = frames.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pred = outputs.argmax(1)\n",
    "            preds.extend(pred.cpu().numpy().tolist())\n",
    "            gts.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    acc = np.mean(np.array(preds) == np.array(gts))\n",
    "    f1 = f1_score(gts, preds, average=\"macro\")\n",
    "    return total_loss / len(val_loader), acc, f1\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Train Loop\n",
    "# ==========================\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n===== EPOCH {epoch+1}/{EPOCHS} =====\")\n",
    "\n",
    "    train_loss = train_one_epoch()\n",
    "    val_loss, val_acc, val_f1 = validate()\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch+1:02d}] \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc*100:.2f}% | \"\n",
    "        f\"Val F1: {val_f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"num_frames\": NUM_FRAMES,\n",
    "                \"img_size\": IMG_SIZE,\n",
    "                \"num_classes\": NUM_CLASSES,\n",
    "            },\n",
    "            SAVE_PATH,\n",
    "        )\n",
    "        print(f\"üî• Best Model Saved: {SAVE_PATH} (F1={best_f1:.4f})\")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(f\"Best F1: {best_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
